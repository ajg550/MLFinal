{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final\n",
    "By Angie Gonzalez & Eric Kwok\n",
    "\n",
    "Facial recognition is something that is very important for a number of reasons, good and bad. Facial recognition is used to identify people via video surveillance, to store into a database for later usage, to spot criminals, and on social media so we have less work in doing “tagging” people. In our experience, we’ve noticed that facial recognition on Facebook can be problematic when trying to differentiate one darked-skin individual to another. We can guess that this can be due to many causes - lack of diversity in those creating the algorithms, lack of adjustment for bad lighting or busy backgrounds (this is also due to the lack of knowledge that people have for lighting black and brown skin tones), the lack of diversity in datasets, or simply, that no one creating the algorithms thought of it as an issue as yet because the field is predominantly white. \n",
    "\n",
    "To tackle this issue, we try to create a model to better differentiate darked and mixed skin individuals. We received our data from this website: http://wiki.cnbc.cmu.edu/Face_Place \n",
    "\n",
    "We downloaded the African-American and Multipath .zip files from the file downloads section right into the working directory. From here, we only kept 74 files (out of over 900 files) of African-American faces and 50 files (out of over 400 files) of Multiracial faces to use for this project. These were files that ended in 'OOF', which only include .jpg files that are the front view of a person, so we could specifically compare and classify facial features. The files were split into 'test' and 'train' directories, with two more directories within them titled 'black' and 'multiracial' (the two classes in this lab).\n",
    "\n",
    "We did want to look into boosting features, such as highlighting cheeks and other structures, but we were not able to import stronger libraries to do this work. To look into solving the facial recognition of people with darker skin, we looked into VGG16 and VGGface to create models for classifying individuals based on skin tone and see how accurately these models can differentiate individuals. By adjusting the models and optimzers, we are able to create a model with high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122 faces in this dataset. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import IPython.display as dp\n",
    "from PIL import Image\n",
    "\n",
    "# Import dataset from local directory \n",
    "fs = !ls *.jpg\n",
    "\n",
    "# Create list of images\n",
    "img = []\n",
    "\n",
    "for ea in fs:\n",
    "    img.append( np.array( Image.open(ea)))\n",
    "\n",
    "num_people, mtx_x, mtx_y, RGB_num = np.shape(img)\n",
    "\n",
    "print('There are %d faces in this dataset. ' % num_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we look at a random image and get its shape to use for the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250, 3)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = img[100]\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras, we attempt to classify the training and test data using VGG16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select 'xception' or 'vgg16'\n",
    "pre_trained = 'vgg16'\n",
    "\n",
    "# Load appropriate packages\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "if pre_trained == 'xception':\n",
    "    from keras.applications.xception import decode_predictions, preprocess_input\n",
    "elif pre_trained == 'vgg16':\n",
    "    from keras.applications.vgg16 import decode_predictions, preprocess_input    \n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow = 250\n",
    "ncol = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Load the VGG16 network\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "input_shape = (nrow, ncol, 3)\n",
    "base_model = applications.VGG16(weights='imagenet',include_top=False,input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))  \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(1,activation = 'sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 250, 250, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, there are 0 trainable paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './train'\n",
    "batch_size = 32\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        train_data_dir,\n",
    "                        target_size=(nrow,ncol),\n",
    "                        batch_size=batch_size,\n",
    "                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = './test'\n",
    "batch_size = 32\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                        test_data_dir,\n",
    "                        target_size=(nrow,ncol),\n",
    "                        batch_size=batch_size,\n",
    "                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_per_epoch =  train_generator.n // batch_size\n",
    "validation_steps =  test_generator.n // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 91s 91s/step - loss: 0.8911 - acc: 0.2759 - val_loss: 8.9676 - val_acc: 0.4375\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 76s 76s/step - loss: 9.4658 - acc: 0.4062 - val_loss: 9.9640 - val_acc: 0.3750\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 73s 73s/step - loss: 9.8953 - acc: 0.3793 - val_loss: 15.9424 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 77s 77s/step - loss: 9.4658 - acc: 0.4062 - val_loss: 10.4622 - val_acc: 0.3438\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 74s 74s/step - loss: 9.8953 - acc: 0.3793 - val_loss: 9.9640 - val_acc: 0.3750\n"
     ]
    }
   ],
   "source": [
    "nepochs = 5  # Number of epochs\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nepochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n",
    "\n",
    "model.save_weights('test_run.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36938\n"
     ]
    }
   ],
   "source": [
    "acc_1 = (0.2759+0.4062+0.3793+0.4062+0.3793)/5\n",
    "print(acc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see the VGG16 model with 0 trainable parameters does not work well and has low accuracy (36.9%). Now, still using keras, we attempt to classify the training and test data using VGG16 with trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "model2 = Sequential()\n",
    "base_model2 = applications.VGG16(weights='imagenet',include_top=False,input_shape=(250,250, 3))\n",
    "\n",
    "for layer in base_model2.layers:\n",
    "    model2.add(layer)\n",
    "    \n",
    "    for layer in base_model2.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 250, 250, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.add(Flatten())\n",
    "model2.add(Dense(256,activation='relu'))  \n",
    "model2.add(Dropout(0.5)) \n",
    "model2.add(Dense(1,activation = 'sigmoid')) \n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, there are now trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 84s 84s/step - loss: 0.4115 - acc: 0.8438 - val_loss: 6.5480 - val_acc: 0.5938\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 78s 78s/step - loss: 6.6696 - acc: 0.5862 - val_loss: 7.0517 - val_acc: 0.5625\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 79s 79s/step - loss: 6.6696 - acc: 0.5862 - val_loss: 6.5480 - val_acc: 0.5938\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 88s 88s/step - loss: 6.0443 - acc: 0.6250 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 88s 88s/step - loss: 6.0443 - acc: 0.6250 - val_loss: 5.0369 - val_acc: 0.6875\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nepochs = 5  # Number of epochs\n",
    "\n",
    "model2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nepochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n",
    "\n",
    "model2.save_weights('vgg_test_run.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65324\n"
     ]
    }
   ],
   "source": [
    "acc_2 = (0.8438+0.5862+0.5862+0.6250+0.6250)/5\n",
    "\n",
    "print(acc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for this VGG16 model was higher (65.3%), but still not as accurate as we aim for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our facial recognition system cannot differentiate between Black and multi-racial people in a basic binary classification system. So, we're going to try VGGFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 250, 250, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 14,846,273\n",
      "Trainable params: 131,585\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "model3 = Sequential()\n",
    "base_model3 = VGGFace(include_top=False, input_shape=(250,250, 3), pooling='avg')\n",
    "\n",
    "for layer in base_model3.layers:\n",
    "    model3.add(layer)\n",
    "    \n",
    "    for layer in base_model3.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "model3.add(Dense(256,activation='relu'))  \n",
    "model3.add(Dropout(0.5)) \n",
    "model3.add(Dense(1,activation = 'sigmoid')) \n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 87s 87s/step - loss: 0.6770 - acc: 0.5938 - val_loss: 0.6699 - val_acc: 0.7500\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 72s 72s/step - loss: 0.6581 - acc: 0.8966 - val_loss: 0.6628 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 72s 72s/step - loss: 0.6516 - acc: 0.8966 - val_loss: 0.6269 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 76s 76s/step - loss: 0.6695 - acc: 0.6562 - val_loss: 0.6177 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 72s 72s/step - loss: 0.6375 - acc: 0.8276 - val_loss: 0.6037 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=nepochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps)\n",
    "\n",
    "model3.save_weights('vgg_test_run2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77416\n"
     ]
    }
   ],
   "source": [
    "total_acc = (0.5938+0.8966+0.8966+0.6562+0.8276)/5\n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using VGGFace model and the adam optimizer help to give greater accuracy. Over 5 epochs, the accuracy is 77.4%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
